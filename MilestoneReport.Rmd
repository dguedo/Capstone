--
title: "Capstone: Milestone Report"
output: html_document
---
**March 15, 2016**

```{r, echo=FALSE}
## Securing the Data
setwd("~/GitHub/Capstone")
```

## Overview
This is the milestone report for the Data Science Capstone Project.

The goal of this report was to build a simple model for the relationship between words, as a first step in creating a predictive text mining application. 

The following sections describe my methods for analysing the datasets

## Libraries

Load the necessary libraries

```{r libraries, warning=FALSE, message=FALSE}
library(tm)
library(knitr)
```

## Securing the Data and Preliminary Analyses

The dataset can be downloaded from here

* https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

Included are three different data files containing text sampled from blogs, news articles, and twitter feeds. The the English versions was used.

```{r load, cache=TRUE, warning=FALSE}
## Read in the files
blog    <- readLines("data/final/en_US/en_US.blogs.txt")
news    <- readLines("data/final/en_US/en_US.news.txt")
twitter <- readLines("data/final/en_US/en_US.twitter.txt")
```

Combined the datasets contain several million lines of text, with over 20 million characters.

```{r stats}
kable(data.frame(
  "Data File"       = c("Blogs", "News", "Twitter"), 
  "Line Count"      = c(length(blog), length(news), length(twitter)),
  "Character Count" = c( sum(nchar(blog)), sum(nchar(news)), sum(nchar(twitter)))
  ))
```

## Clean and sample the data sets

In order to be able to predict the next word with the highest degree of accuracy, in a reasonably efficient manner, the data set needed to be cleaned up. Numbers, punctuation, special characters, and stop words were removed.  In addition, words where converted back to their stems. 

The next sections uses the tm package, you can find an introduction here
* https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

Samples of the data sets are used to reduce the memory footprint.

```{r sample, cache=TRUE, warning=FALSE}
set.seed(90210)

s_blog   <- sample(blog, size = 1000, replace = TRUE)
s_news   <- sample(news, size = 1000, replace = TRUE)
s_twiter <- sample(twitter, size = 1000, replace = TRUE)

# combine and encode
combined <- c(s_blog, s_news, s_twiter)
Encoding(combined) <- "latin1"
combined <- iconv(combined, "latin1", "ASCII", "") 

# create the corpus
swiftkey = Corpus(VectorSource(combined))

# clean up objects
rm(blog, news, twitter, s_blog, s_news, s_twiter, combined)
```

Tidy up the data sets by removing elements and converting to a common case.

```{r tidy, cache=TRUE}
swiftkey <- tm_map(swiftkey, content_transformer(tolower))

# remove the most commonly used words in the english language
swiftkey <- tm_map(swiftkey, removeWords, stopwords("english"))

# reduce inflected (or sometimes derived) words to their word stem
swiftkey <- tm_map(swiftkey, stemDocument)

# clean up
swiftkey <- tm_map(swiftkey, stripWhitespace)
swiftkey <- tm_map(swiftkey, removePunctuation)
swiftkey <- tm_map(swiftkey, removeNumbers)
```

## Basic n-gram models

```{r}
ff <- TermDocumentMatrix(swiftkey)
findFreqTerms(ff, lowfreq=100)
```


## end with word cloud for fun

