---
title: "Capstone: Milestone Report"
output: html_document
---
**March 15, 2016**

```{r, echo=FALSE}
## Securing the Data
setwd("~/GitHub/Capstone")
```

## Overview
This is the milestone report for the Data Science Capstone Project.

The goal of this report was to build a simple model for the relationship between words, as a first step in creating a predictive text mining application. 

The following sections describe my methods for analysing the datasets

## Libraries

Load the necessary libraries

```{r libraries, warning=FALSE, message=FALSE}
library(tm)
library(knitr)
```

## Securing the Data and Preliminary Analyses

The dataset can be downloaded from here

* https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

Included are three different data files containing text sampled from blogs, news articles, and twitter feeds. I used the the English versions.

```{r load, cache=TRUE, warning=FALSE}
## Read in the files
blog    <- readLines("data/final/en_US/en_US.blogs.txt")
news    <- readLines("data/final/en_US/en_US.news.txt")
twitter <- readLines("data/final/en_US/en_US.twitter.txt")
```

The datasets are fairly large with several million lines of text.

```{r stats}
kable(data.frame(
  "Data File"       = c("Blogs", "News", "Twitter"), 
  "Line Count"      = c(length(blog), length(news), length(twitter)),
  "Character Count" = c( sum(nchar(blog)), sum(nchar(news)), sum(nchar(twitter)))
  ))
```

## Clean and sample the data sets

We need to clean the data sets of any characters that will impede our ability to predict the next word. Such as numbers, punctuation and special characters. 

The next sections will be using the tm package, you can find a good introduction here
* https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

Samples of the data sets are used to reduce the memory footprint.

```{r sample, cache=TRUE, warning=FALSE}
set.seed(90210)

s_blog   <- sample(blog, size = 10000, replace = TRUE)
s_news   <- sample(news, size = 10000, replace = TRUE)
s_twiter <- sample(twitter, size = 10000, replace = TRUE)

swiftkey = Corpus(VectorSource(c(s_blog, s_news, s_twiter)))
```

Tidy up the data sets by removing elements and converting to a common case.

```{r tidy, cache=TRUE}
swiftkey <- tm_map(swiftkey, content_transformer(tolower))

# remove the most commonly used words in the english language
swiftkey <- tm_map(swiftkey, removeWords, stopwords("english"))

# reduce inflected (or sometimes derived) words to their word stem
swiftkey <- tm_map(swiftkey, stemDocument)

# clean up
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
```


move to n-gram model

end with word cloud for fun

